{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3oxGt5IIKhO",
    "outputId": "2b49e045-6504-482a-e66b-815af0865150"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRRpYII_eTMz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7ce682cd-5de6-45c8-d314-9d297d0fe51a"
   },
   "outputs": [],
   "source": "import os\n\nfolder_path = \"/content/drive/MyDrive/dm project\"\n\nprint(os.listdir(folder_path))\n\n\n\nimport os\nimport re\n\n\ndef clean_numbers_and_symbols(text):\n    text = re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", text)\n    text = re.sub(r\"[-/]\", \"\", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n\ndef remove_empty_lines(lines):\n    return [line for line in lines if line.strip()]\n\nfolder_path = \"/content/drive/MyDrive/dm project\"\ncleaned_folder_path = \"/content/drive/MyDrive/dm_project_cleaned\"\nos.makedirs(cleaned_folder_path, exist_ok=True)\n\nfor file_name in os.listdir(folder_path):\n    if file_name.endswith(\".txt\"):\n        file_path = os.path.join(folder_path, file_name)\n\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            lines = file.readlines()\n\n        cleaned_lines = [clean_numbers_and_symbols(line.strip()) for line in lines]\n\n\n        cleaned_lines = remove_empty_lines(cleaned_lines)\n\n        cleaned_file_path = os.path.join(cleaned_folder_path, file_name)\n        with open(cleaned_file_path, \"w\", encoding=\"utf-8\") as cleaned_file:\n            cleaned_file.write(\"\\n\".join(cleaned_lines))\n\nprint(\"All files have been cleaned and saved in:\", cleaned_folder_path)\n\n\n\nimport os\n\n# Lowercase Conversion\nfor filename in os.listdir():\n    if os.path.isfile(filename):\n            with open(filename, \"r+\", encoding=\"utf-8\") as f:\n                content = f.read()\n                f.seek(0)  # Move file pointer to the beginning\n                f.write(content.lower())\n                f.truncate()  # Remove any remaining original content\n\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# Ensure NLTK data is downloaded\nnltk.download('punkt', quiet=True)\nnltk.download('punkt_tab', quiet=True)\n\n# Word Tokenization\nfor filename in os.listdir(): # يمر على الفايلز\n    if os.path.isfile(filename):# يعالج الفايلز فقط\n\n            with open(filename, \"r\", encoding=\"utf-8\") as f:#يفتحهم ةيقراهم\n                content = f.read()\n\n            # Tokenize words\n            tokens = word_tokenize(content) # Fixed: Corrected indentation to align with the 'try' block.\n\nimport os\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords', quiet=True)\nstop_words = set(stopwords.words(\"english\"))\n\n# يمر على الفايلز\nfor file_name in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, file_name)\n    if file_name.endswith(\".txt\"):\n        with open(file_path, \"r+\", encoding=\"utf-8\") as file:\n\n            file_content = file.read()\n\n            filtered_content = \" \".join(word for word in file_content.split() if word.lower() not in stop_words) # Remove stop words\n\n\n            file.seek(0)\n            file.write(filtered_content)\n            file.truncate()  # Remove any remaining original content\n\n\n\nimport os\nimport nltk\nfrom nltk.stem import SnowballStemmer\n\nnltk.download(\"punkt\")\nstemmer = SnowballStemmer(\"english\")\n\ndef apply_stemming(text):\n    words = nltk.word_tokenize(text)\n    stemmed_words = [stemmer.stem(word) for word in words]\n    return \" \".join(stemmed_words)\n\nfolder_path = \"/content/drive/MyDrive/dm_project_cleaned\"\nstemmed_folder_path = \"/content/drive/MyDrive/dm_project_stemmed\"\nos.makedirs(stemmed_folder_path, exist_ok=True)\n\nfor file_name in os.listdir(folder_path):\n    if file_name.endswith(\".txt\"):\n        file_path = os.path.join(folder_path, file_name)\n\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            lines = file.readlines()\n\n        stemmed_lines = [apply_stemming(line.strip()) for line in lines]\n\n        stemmed_file_path = os.path.join(stemmed_folder_path, file_name)\n        with open(stemmed_file_path, \"w\", encoding=\"utf-8\") as stemmed_file:\n            stemmed_file.write(\"\\n\".join(stemmed_lines))\n\nprint(\"All files have been stemmed and saved in:\", stemmed_folder_path)\n\n\n\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\n\nfolder_path =\"/content/drive/MyDrive/dm_project_stemmed\"\ndirectory = folder_path\n\ndocuments = []\n\nfor filename in os.listdir(directory):\n\n    if filename.endswith('.txt'):  #يتاكد من نوع الملف الي نبيه\n        file_path = os.path.join(directory, filename)  # Full path to the file\n\n        # Check if it's a valid file\n        if os.path.isfile(file_path):\n            with open(file_path, 'r') as file:\n                # Read the content of the file and append it to the documents list\n                documents.append(file.read())  # Read the entire file content\n                print(f\"Successfully read {filename}\")# will print the files that have been read!!\n\n\nvectorizer = TfidfVectorizer()  #الي بيسوي لنا المايتركس  tf-idf\ntfidf_matrix = vectorizer.fit_transform(documents)  # ندخله على الدوكمنتس\n\n# يجيب التيرمز\nfeature_names = vectorizer.get_feature_names_out()\n\n# Fit and transform the text data into a TF-IDF matrix\ntfidf_matrix = vectorizer.fit_transform(documents)\n\n# Convert TF-IDF matrix to a dense array and print it\ntfidf_array = tfidf_matrix.toarray()\nfeature_names = vectorizer.get_feature_names_out()\n\n# Convert TF-IDF matrix to a Pandas DataFrame\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n\n# Print the DataFrame to see words instead of numbers\n\nprint(tfidf_df)\nprint(\"\\nTF-IDF Matrix:\")\nprint(tfidf_matrix.toarray())\n\nimport os\n\n# المسار إلى الملفات\nfolder_path =\"/content/drive/MyDrive/dm_project_stemmed\"# أو أي مسار حفظت فيه الملفات\nfile_list = sorted(os.listdir(folder_path))\ndocuments = []\n\nfor file_name in file_list:\n    if file_name.endswith(\".txt\"):\n        with open(os.path.join(folder_path, file_name), 'r', encoding='utf-8') as file:\n            documents.append(file.read())\n\n# الآن، هذه قائمة أسماء الملفات (المعاني) بالترتيب\ndocument_names = [file_name for file_name in file_list if file_name.endswith(\".txt\")]\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Step 1: Define your query (choose something meaningful from your topic)\nquery = \"delicious food and good service \"  # or try a book-related one\n\n# Step 2: Transform the query using the SAME vectorizer\nquery_vector = vectorizer.transform([query])\n\n# Step 3: Compute cosine similarity between the query and all documents\nsimilarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n\n# Step 4: Get top matches (optional: sort and print top 5 similar docs)\ntop_indices = similarities.argsort()[::-1]  # Sort by descending similarity\n\nprint(\"\\nTop matching documents for the query:\")\nfor idx in top_indices[:30]:\n    score = similarities[idx]\n    if score > 0:\n        print(f\"{document_names[idx]} (Score: {score:.4f})\")\n"
  },
  {
   "cell_type": "code",
   "source": "# Install Gradio\n\nfrom PIL import Image\nimport gradio as gr\n\n# Load image\nlogo = Image.open(\"/content/IMG_A57A1B33449D-1.jpeg\")\n",
   "metadata": {
    "id": "XeqrLZX8McR5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "outputId": "a2d38ade-2b77-49dc-f8b6-09755d2c3e11"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Install Gradio\n\nfrom PIL import Image\nimport gradio as gr\n\n# Load image\nlogo = Image.open(\"/content/IMG_A57A1B33449D-1.jpeg\")\n\n\n# Define the search function using TF-IDF and cosine similarity\ndef search_documents(query):\n    query_vector = vectorizer.transform([query])\n    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n    top_indices = similarities.argsort()[::-1]\n    results = []\n    for idx in top_indices[:5]:  # Show top 5 results only\n        score = similarities[idx]\n        if score > 0:\n            results.append(f\"{document_names[idx]} (Score: {score:.4f})\")\n    return \"\\n\".join(results)\n\n# Build the Gradio interface\nwith gr.Blocks() as demo:\n    # Display logo at the top with medium size\n    gr.Image(value=logo, label=\"\", show_label=False, show_download_button=False, height=150, width=150)\n\n    # Heading and input box\n    gr.Markdown(\"## Enter your query to retrieve the most similar documents\")\n\n    query_input = gr.Textbox(lines=2, placeholder=\"Type your query here\")\n    output_text = gr.Textbox(label=\"Top related Documents\")\n\n    # Search button\n    search_button = gr.Button(\"Search\")\n    search_button.click(fn=search_documents, inputs=query_input, outputs=output_text)\n\n# Launch the interface\ndemo.launch(share=True, debug=True)\n",
   "metadata": {
    "id": "efvi4hCnNzXq",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "e98b5f3b-f025-41a1-ffdc-e1629f921df4"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}